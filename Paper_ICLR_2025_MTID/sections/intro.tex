\section{Introduction}

Recently, procedure planning has exhibited critical reasoning capability for solving real-world challenges in complex domains, such as robotic navigation~\citep{sermanet2024robovqa,bhaskara2024trajectory} and autonomous driving~\citep{wang2024driving,liao2024bat}. 
Among them, procedure planning in instructional videos~\citep{zhao2022p3iv,wang2023pdpp,li2023skip} has been widely concerned because of its wide application scenarios, which involve identifying and generating coherent action sequences that align with the task's objectives, given the start and end visual observations.

% 说明各个方法的核心
In the field of procedure planning in instructional videos, the primary challenge lies in modeling the temporal evolution mechanism among actions and identifying pertinent conditions that can effectively steer the generation of intermediary actions in scenarios where information is scarce. 
As depicted in \Cref{fig:task}(a), many scholars have resorted to capturing different forms of auxiliary information about the intermediate states to bridge the gap between observed states and unobserved actions. 
% example
For example, event-based supervision~\citep{wang2023event} leverages key task events to help the model learn temporal action structures, while task label supervision~\citep{wang2023pdpp} uses task-specific labels for better alignment with the task objective. The probabilistic procedure knowledge graph~\citep{nagasinghe2024not} provides structured knowledge to enhance the understanding of action dependencies. Additionally, \citet{niu2024schema} leverage large language models (LLMs) to describe state changes, improving the model’s grasp of causal relationships by combining visual and language descriptions.
% problem
%However, these methods rely on text-level supervision instead of visual-level feature supervision, leading to less detailed and comprehensive information. 
However, all these methods are limited to providing text-level supervision, resulting in less detailed and comprehensive information, and failing to precisely capture the temporal relationships between actions. 
Additionally, these methods decouple the acquisition of supervisory information from the intermediate action reasoning process, hindering effective collaboration and interaction between the two. Consequently, it becomes challenging to fully integrate and adapt to current action reasoning tasks. 

\input{figures/task}

%%%%%%%%%%%
% 这个是一个总括，波波倒数第二段
%%%%%%%%%%%%
Based on the above analysis, we propose the Masked Temporal Interpolation Diffusion (MTID) model for procedure planning in instructional videos. 
As shown in \Cref{fig:task}(b), the core concept is to leverage intermediate latent visual features, generated synchronously by a latent space temporal interpolation module, to provide comprehensive visual-level information for mid-state supervision. 
In the meanwhile, the generated visual features are directly injected into the action reasoning model, ensuring the generation of intermediate supervision information can be effectively applied to the current action reasoning task through end-to-end training.

%%%%%%%%%%%%%%
% 总结我们的主要贡献
%%%%%%%%%%%
% 构成
Specifically, \textbf{MTID} comprises three core components: a task classifier, a latent space temporal interpolation module, and a diffusion model framework that integrates both a U-Net and a DDIM strategy. \textcolor{orange}{Additionally, \textbf{MTID} inherits the input matrix, conditional projection, U-Net model, and MSE loss from PDPP.}
% first
In the first stage, a transformer-based classifier predicts the task class label $c$ for the entire instructional video, given the start and end observations. This prediction serves as the foundation for subsequent reasoning and action generation.
% second
The latent space temporal interpolation module is designed to capture and model temporal relationships. 
% 1
It employs an observation encoder to transform the visual features of the observations into latent features that maintain temporal dependencies and discard object details. 
% 2
A latent space interpolator then generates intermediate features using a learnable interpolation matrix, which dynamically adjusts the interpolation ratio to fit task-specific requirements. 
% 3
These interpolated features are refined through transformer blocks, enhancing their temporal coherence and capturing the dependencies between action sequences.
% third
In the third stage, during the denoising phase for generating action sequences, the input matrix is constructed by concatenating the task class label, observed visual features, and Gaussian noise sampled from $\mathcal{N}(0, I)$. 
% mask
A masked projection is applied to exclude irrelevant actions, ensuring that the generated actions remain within the desired range. 
% ddim
To accelerate inference, DDIM is used throughout the iterative process.
% loss
To further ensure task relevance, a task-adaptive masked proximity loss is introduced. This loss function gradually decreases its focus toward the central features, reinforcing supervision on intermediate latent features while penalizing irrelevant actions, thereby constraining the generation process. 
% all
By leveraging detailed information from both the start and end observations $V_s$ and $V_g$, our model accurately predicts target action sequences, as demonstrated by experimental results on the CrossTask, COIN, and NIV datasets.

The main contributions of this paper are as follows:
\begin{itemize}[leftmargin=*]
\vspace{-2mm}
    \item We propose a Masked Temporal Interpolation Diffusion model with a mask to limit action initialization and a task-adaptive masked proximity loss to enhance accuracy.
    
    \item We use a latent space temporal interpolation module to extract intermediate visual features with temporal relationships from the start and end states to guide the diffusion process.
    
    \item Extensive experiments are conducted on several widely used benchmarks, showing significant performance improvements on multiple tasks using the proposed method.
\end{itemize}



