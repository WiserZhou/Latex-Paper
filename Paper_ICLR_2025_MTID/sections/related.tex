\section{Related Work}
\label{gen_inst}
\textbf{Procedure Planning in Instructional Videos.} Procedure planning involves generating goal-directed action sequences from visual observations in unstructured videos. Our work builds on PDPP~\citep{wang2023pdpp}, which models action sequences using diffusion processes. Earlier approaches focused on learning sequential latent spaces~\citep{chang2020procedure} and adversarial policy learning~\citep{bi2021procedure}. Recent methods introduced linguistic supervision for step prediction~\citep{zhao2022p3iv}, mask-and-predict strategies for step relationships~\citep{wang2023event}, and breaking sequences into sub-chains by skipping unreliable actions~\citep{li2023skip}. KEPP~\citep{nagasinghe2024not} incorporated probabilistic knowledge for step sequencing, while SCHEMA~\citep{niu2024schema} tracked state changes at each step. However, none of these methods focus on the visual-level temporal logic between actions. Our approach introduces mid-state temporal supervision to capture these relationships, resulting in more accurate and efficient predictions.


\textbf{Diffusion Probabilistic Models for Long Video Generation.} 
Recent advances in diffusion probabilistic models~\citep{croitoru2023diffusion}, originally popularized for image generation~\citep{rombach2022high}, have achieved significant progress in generating long video sequences~\citep{weng2024art,zhou2024upscale,jiang2024videobooth}. StreamingT2V~\citep{henschel2024streamingt2v} excels in producing temporally consistent long videos with smooth transitions and high frame quality, overcoming the typical limitations of short video generation. StoryDiffusion~\citep{zhou2024storydiffusion} further enhances sequence coherence through consistent self-attention, enabling the creation of detailed, visually coherent stories. These innovations address challenges like maintaining temporal coherence and generating realistic motion, inspiring the implementation of auxiliary temporal coherence mechanisms in our approach.
