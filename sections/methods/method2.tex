\subsection{MTID: Masked Temporal Interpolation Diffusion}
\label{method2} 
Our approach consists of three main stages: predicting the task class, obtaining latent temporal interpolation supervision to guide the logical sequence, and modeling the distribution of action sequences. Unlike \citet{wang2023pdpp}, the first stage involves solving a standard classification problem using a multi-head self-attention transformer to extract features from observations and transform them into class labels. This process includes linear transformations, ReLU activations, and dropout layers to enhance the model's expressiveness and generalization capabilities.

The second stage leverages \(V_{start}\) and \(V_{goal}\) as inputs for the latent space temporal interpolation module to obtain the latent frames needed to solve \(p(\upsilon_{1:n} \mid V_{start}, V_{goal})\). These frames are then used for cross-attention within the residual blocks of the U-Net architecture. During the cross-attention calculation, the latent information obtained from the interpolation module serves as conditioning information to adjust the intermediate matrices.

The third stage focuses on modeling \(p(a_{1:T} \mid \upsilon_{1:n}, c, V_{start}, V_{goal})\) to effectively handle the procedure planning task.

\input{sections/methods/method_two/first}

\input{sections/methods/method_two/second}

\input{sections/methods/method_two/third}