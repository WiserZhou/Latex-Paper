\subsubsection{Masked Projection for Initial and Iterative Steps}
In a typical diffusion model, the input consists of the data distribution the model is designed to fit, without requiring any additional guidance. However, for our procedure planning task, the distribution we aim to model consists of intermediate action sequences \([a_1, a_2, \dots, a_T]\), which are influenced by initial observations and the task class derived from an earlier learning stage. This requires a method to incorporate these guiding conditions into the diffusion process.

We construct the input matrix \( \hat{x}_i \) by concatenating three components: (1) the visual observations of the start and goal states \((V_s, V_g)\), (2) the predicted task class \( c \), and (3) a sequence of candidate actions \((a_{1:T})\). \citet{wang2023pdpp} incorporate these guiding conditions as supplementary inputs by concatenating them directly to the action features. Specifically, they concatenate the starting observation \( o_s \) with the first action \( a_1 \), and the goal observation \( o_g \) with the last action \( a_T \). This method assumes that the start and goal observations are most relevant to the first and last actions, respectively, providing prior knowledge to the model. Intermediate observations are not used as supervision signals, so their values are set to zero except for the start and goal observations. Thus, the input matrix can be expressed as:
\begin{equation}
    \hat{x}_i = \begin{bmatrix}  
        c & c & \cdots & c & c \\  
        \hat{a}_1 & \hat{a}_2 & \cdots & \hat{a}_{T-1} & \hat{a}_T \\  
        V_s & 0 & \cdots & 0 & V_g
    \end{bmatrix}.
\end{equation}
Inspired by the masked latent modeling scheme and asymmetric masking diffusion transformer introduced by \citet{gao2023masked}, which predict masked tokens from unmasked ones during the diffusion process, we design a masking mechanism to enhance contextual relation learning and limit the scope of action generation.

During initial noise addition, Gaussian noise is introduced exclusively to the unmasked (active) actions. This strategy limits the search space for optimal actions to the task-defined subset, reducing the learning load during loss minimization. As the action space grows, this approach becomes more advantageous, leading to faster convergence and higher accuracy in the denoising phase.

In the iterative steps, for \( \varepsilon_i \in \mathbb{R}^{B \times H \times A} \), a binary mask is appended to the candidate action sequence. This mask is determined by the predicted task class \( c \), with `1' indicating active actions in the predicted task class, and `0' for other actions along the \( A \) dimension. The masked projection is then defined as:
\begin{equation}
    MP(\varepsilon_i, c) = \hat{a}_i,
\end{equation}
where \( B \) denotes batch size, \( H \) denotes horizon steps, \( A \) denotes action dimensions, and \( \varepsilon \) is sampled from random noise. By sampling from random noise, concatenating the guiding conditions, and applying the masked projection, the input matrix \( \hat{x}_i \) is constructed for the diffusion process.

