\subsubsection{Latent Space Temporal Interpolation}
In this subsection, we introduce how our method uses temporal interpolation to guide the iteration of U-Net. The key to addressing the challenge of maintaining the logical continuity and temporal coherence within horizon steps is establishing connections between the actions and observations during the iteration process. 

To solve this problem, we propose the \textit{Temporal Interpolation Predictor}, which encodes visual observations into a latent space that captures temporal and logical relationships, enabling more accurate action predictions. Specifically, our Temporal Interpolation Predictor uses an encoder $E$ with convolutional layers to transform visual observations into a latent temporal space:
\begin{equation}
L_s, L_g = E(V_s, V_g),
\end{equation}
where $L_s$ and $L_g$ represent the latent features of the visual observations $V_s$ and $V_g$.

In the latent space, we employ a transformer-based structure to predict several intermediate latent features. First, the latent features $L_s$ and $L_g$ are linearly interpolated to form a sequence \{ $I_1, I_2, \dots, I_n$ \}, where $n$ is an adjustable hyper-parameter. 

The alpha generation is defined by:
\begin{equation}
\alpha = \sigma(W \cdot \mathbf{1} + b),
\end{equation}
where $\sigma$ is the sigmoid function, $W \in \mathbb{R}^{b \times d}$ is the weight matrix, $\mathbf{1} \in \mathbb{R}^d$ is a vector of ones, $b \in \mathbb{R}^b$ is the bias vector, and $\alpha \in \mathbb{R}^{b \times n}$ represents the alpha values, with $b$ as the batch size and $n$ as the number of blocks.

The interpolation for each frame is computed as:
\begin{equation}
I_i = (1 - \alpha_i) \cdot x_1 + \alpha_i \cdot x_2,
\end{equation}
where $I_i \in \mathbb{R}^{b \times d}$ is the $i$-th interpolated frame, $x_1, x_2 \in \mathbb{R}^{b \times d}$ are the input frames, and $\alpha_i \in \mathbb{R}^{b \times 1}$ is the $i$-th column of $\alpha$. The alpha values control the relative weighting of the two input frames in each interpolated frame.

This sequence is then passed through a series of transformer blocks $B$ to predict the transition latent features:
\begin{equation}
F_1, F_2, \dots, F_n = B(I_1, I_2, \dots, I_n).
\end{equation}
Motivated by \citet{khachatryan2023text2video}, we incorporate cross-attention layers into the residual blocks of a U-Net. We use the transition latent features as keys and values, and the original $\hat{X}_i$ inside the model as queries. The cross-attention is computed as:
\begin{equation}
\hat{X}_i = \text{CrossAttention}(\hat{X}_i, F_i, F_i).
\end{equation}


% \subsubsection{Semantic Motion Predictor for Video Generation}
% We argue that this limitation arises from their reliance on temporal modules, which predict intermediate frames based solely on temporal dynamics, often disregarding spatial information.

% Similar to previous video generation methods, we optimize the model by computing the MSE loss between the predicted video frames $O = (O_1, O_2, \dots, O_L)$ and the ground truth frames $G = (G_1, G_2, \dots, G_L)$:

% \begin{equation}
% \mathrm{Loss} = \mathrm{MSE}(G, O).
% \end{equation}

% By encoding images into a semantic space that captures spatial relationships, our Semantic Motion Predictor effectively models motion information, resulting in smooth transition videos even with large motion. The improvements can be observed in Fig. 1 and Fig. 5.


% \subsubsection{Training-Free Consistent Images Generation}

% We insert Consistent Self-Attention in place of the original self-attention layer in the U-Net architecture of existing image generation models. To maintain the training-free and pluggable nature, we reuse the original self-attention weights. 

% Formally, given a batch of image features $I \in \mathbb{R}^{B \times N \times C}$, where $B$, $N$, and $C$ represent the batch size, number of tokens in each image, and channel number, respectively, we define a function $\mathrm{Attention}(X_k, X_q, X_v)$ to compute self-attention. $X_k$, $X_q$, and $X_v$ represent the key, query, and value matrices used in the attention calculation. The original self-attention is performed independently on each image feature $I_i \in I$. The feature $I_i$ is projected to $Q_i$, $K_i$, $V_i$, and sent into the attention function, yielding:

% \begin{equation}
% O_i = \operatorname{Attention}\left( Q_i, K_i, V_i \right).
% \end{equation}

% To enable interactions among images within a batch for subject consistency, our Consistent Self-Attention samples some tokens $S_i$ from other image features in the batch:

% \begin{equation}
% S_i = \operatorname{RandSample}\left( I_1, I_2, \dots, I_{i-1}, I_{i+1}, \dots, I_B \right),
% \end{equation}

% where $\operatorname{RandSample}$ is the random sampling function. After sampling, we pair the sampled tokens $S_i$ with the image feature $I_i$ to form a new set of tokens $P_i$. We perform linear projections on $P_i$ to generate new key $K_{P_i}$ and value $V_{P_i}$ for Consistent Self-Attention. The original query $Q_i$ remains unchanged. Finally, the self-attention is computed as follows:

% \begin{equation}
% O_i = \operatorname{Attention}\left( Q_i, K_{P_i}, V_{P_i} \right).
% \end{equation}

% By performing self-attention across the batch, our method facilitates interactions among features of different images. This promotes the convergence of characters, faces, and attires during the generation process. Despite its simplicity and training-free nature, our Consistent Self-Attention efficiently generates subject-consistent images, as we will demonstrate in our experiments. These images help narrate a complex story, as shown in Fig. 2. For clarity, we also provide the pseudo code in Algorithm 1.