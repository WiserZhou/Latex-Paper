
\subsection{Evaluation Protocol}
\label{sec:protocal}

\textbf{Datasets.} We evaluate our MTID method on three instructional video datasets: \textbf{CrossTask}~\citep{zhukov2019cross}, \textbf{COIN}~\citep{tang2019coin}, and \textbf{NIV}~\citep{alayrac2016unsupervised}. CrossTask consists of 2,750 videos across 18 tasks, covering 133 actions, with an average of 7.6 actions per video. COIN contains 11,827 videos spanning 180 tasks, with an average of 3.6 actions per video. NIV includes 150 videos from 5 tasks, with an average of 9.5 actions per video. We randomly split each dataset into training (70\% of videos per task) and testing (30\%), following previous works~\citep{sun2022plate, wang2023pdpp, niu2024schema}.


\textbf{Metrics.} Following previous works~\citep{sun2022plate, zhao2022p3iv, wang2023pdpp, niu2024schema, nagasinghe2024not}, we evaluate the models using three key metrics: (1) \textbf{Success Rate (SR)} is the strictest metric, where a procedure is considered successful only if every predicted action step exactly matches the ground truth. (2) \textbf{mean Accuracy (mAcc)} computes the average accuracy of predicted actions at each time step, where an action is deemed correct if it matches the ground truth action at the corresponding time step. (3) \textbf{mean Intersection over Union (mIoU)} quantifies the overlap between the predicted procedure and the ground truth by calculating mIoU as $\frac{|\{a_t\} \cap \{\hat{a}_t\}|}{|\{a_t\} \cup \{\hat{a}_t\}|}$, where $\{a_t\}$ represents the set of ground truth actions, and $\{\hat{a}_t\}$ denotes the set of predicted actions.


