
\subsection{Evaluation Protocol}
\label{sec:protocal}
\textbf{Datasets.} We evaluate our MTID method on three instructional video datasets: \textbf{CrossTask}~\citep{zhukov2019cross}, \textbf{COIN}~\citep{tang2019coin}, and \textbf{NIV}~\citep{alayrac2016unsupervised}. CrossTask contains 2,750 videos from 18 tasks, illustrating 133 actions, with an average of 7.6 actions per video. COIN has 11,827 videos across 180 tasks, averaging 3.6 actions per video. NIV includes 150 videos of 5 tasks, with an average of 9.5 actions per video. We randomly split 70\% of videos per task for training and 30\% for testing, following prior work~\citep{sun2022plate, wang2023pdpp, niu2024schema}.

We extract step sequences $a_{t:(t+T-1)}$ from the videos, where the horizon $T$ ranges from 3 to 6. Following~\citep{zhao2022p3iv,wang2023pdpp}, we generate action sequences $\{[a_i, ..., a_{i+T-1}]\}_{i=1}^{n-T+1}$ by sliding a window of size $T$ over the $n$ actions. For each sequence, the video clip feature at the start of action $a_i$ is used as the start observation $o_s$, and the feature at the end of $a_{i+T-1}$ as the goal state $o_g$, with both clips lasting 3 seconds.

For CrossTask, we use two types of pre-extracted features: (1) dataset-provided 3200-dimensional features combining I3D, ResNet-152, and audio VGG~\citep{carreira2017quo,he2016deep,hershey2017cnn}, and (2) features from a HowTo100M-trained encoder~\citep{miech2019howto100m}, as in~\citep{wang2023pdpp}. For COIN and NIV, we use HowTo100M features~\citep{wang2023pdpp} for consistency and fair comparison.

\textbf{Metrics.} Following previous works~\citep{sun2022plate, zhao2022p3iv, wang2023pdpp, niu2024schema, nagasinghe2024not}, we evaluate the models using three key metrics. (1) \textbf{Success Rate (SR)} is the strictest metric, where a procedure is deemed successful only if every predicted action step matches the ground truth exactly. (2) \textbf{mean Accuracy (mAcc)} calculates the average accuracy of predicted actions at each time step, with an action considered correct if it matches the ground truth action at the corresponding time step. (3) \textbf{mean Intersection over Union (mIoU)} measures the overlap between the predicted procedure and the ground truth by computing the mIoU as $\frac{|\{a_t\} \cap \{\hat{a}_t\}|}{|\{a_t\} \cup \{\hat{a}_t\}|}$, where $\{a_t\}$ is the set of ground truth actions and $\{\hat{a}_t\}$ is the set of predicted actions.

Previous works~\citep{chang2020procedure, bi2021procedure, sun2022plate} computed the mIoU metric over mini-batches, averaging the results across the batch size. However, this method introduces variability based on the batch size. For example, if the batch size is set to the size of the entire dataset, all predicted actions may be deemed correct. In contrast, using a batch size of one penalizes any mismatch between predicted and ground-truth sequences. To address this issue, we follow \citet{wang2023pdpp} by standardizing mIoU calculation, computing it for each individual sequence and then averaging the results, which effectively treats the batch size as one.

\textbf{Baselines.} We follow previous works and consider the following baseline methods for comparisons. The recent baselines are (1) PlaTe~\cite{sun2022plate}, which extends DNN and uses a Transformer-based architecture; (2) Ext-GAIL~\cite{bi2021procedure}, which uses reinforcement learning for procedure planning; (3) P$^3$IV~\cite{zhao2022p3iv}, which is the first to use weak supervision and proposed a generative adversarial framework; (4) PDPP~\cite{wang2023pdpp}, which is a diffusion-based model for sequence distribution modeling; (5) EGPP~\cite{wang2023event}, which extracts event information for procedure planning. (6) KEPP~\cite{nagasinghe2024not}, which proposes a knowledge-enhanced procedure planning system using a probabilistic procedural knowledge graph to improve strategic planning; and (7) SCHEMA~\cite{niu2024schema}, which structures the state space by tracking state changes and using large language model-generated descriptions. Details of other earlier baselines can be found in appendix.