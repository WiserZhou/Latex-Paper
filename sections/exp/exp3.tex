\subsection{Results}
\input{tables/classifier} 
\textbf{Results for Task Classifier}. The first stage of our learning is to predict the task class with the given start and goal observations. We implement this with transformer models to replace the two-layer Res-MLP which is used in \citet{wang2023pdpp} and train it with a simple CE loss. The classification results for different planning horizons on three datasets are shown in \cref{tab:classifier}. We can see that our method surpasses previous methods across all aspects. Our model achieves an improvement of approximately 2\% in task classification accuracy on the COIN dataset (with the largest task space). It also achieves a significant improvement on CrossTask; and maintains perfect accuracy (100\%) on NIV as in other configurations.
\input{tables/crosstask}

\textbf{Comparisons on CrossTask.} We evaluate performance on CrossTask using four prediction horizons that are commonly used in previous work, with results presented in \cref{tab:crosstask}. Features extracted by the HowTo100M-trained encoder are denoted with $\dagger$, while other features are provided directly by CrossTask. It is important to note that we compute the mean Intersection over Union (mIoU) by averaging the IoU values for each action sequence individually, rather than for a mini-batch, as described in \cref{sec:protocal}. This method may produce higher mIoU values compared to mini-batch calculations. The results in \cref{tab:crosstask} indicate that our method outperforms all other approaches across all metrics, except for the success rate (SR) at $T = 6$, where our model ranks second. Specifically, for $T = 3$ on CrossTask, our method achieves approximately 1.5\% higher SR than SCHEMA (40.45 vs. 38.93) and about 2.3\% higher than KEPP (40.45 vs. 38.12). These improvements are consistent across longer prediction horizons ($T = 4, 5, 6$) and other step-level metrics, including mAcc and mIoU. 
\input{tables/coin}
\input{tables/niv}

\textbf{Comparisons on NIV and COIN.} \cref{tab:coin} and \cref{tab:niv} present our evaluation results on the NIV and COIN datasets, showing that our approach consistently outperforms or matches the best-performing methods across both datasets. Specifically, on the NIV dataset, where both SR and mIoU are relatively high, our model increases SR by over 2\% for both horizons and surpasses the previous best mIoU by more than 6\% when $T=4$. For the large COIN dataset, where mAcc is comparatively lower, our model achieves a significant improvement of over 3\% in mAcc and more than 6\% in mIoU. These results demonstrate that our model performs strongly across datasets of varying sizes.

\input{tables/ablation_classifier}
\textbf{Ablation Studies.} We begin by performing ablation studies across three datasets to assess the impact of our transformer-based classifier. As illustrated in \cref{tab:classifier}, the introduction of our transformer-based classifier significantly enhances the performance of PDPP. While the improvements are modest for longer horizons, this demonstrates the effectiveness of our temporal interpolation module.



% We first conduct ablation studies on CrossTask to validate the effect of two key components, state alignment (Eq.~4 and Figure~4(a)) and mid-state prediction (Sec.~3.3.2). As shown in Table~4, visual-language alignment improves SR by 1.4\%â€“2.4\% ((c) vs. (a), (d) vs. (b)) for $T = 3$ on CrossTask. Additionally, the mid-state prediction module enhances performance, with the improvements becoming more significant when combined with state alignment (i.e., +0.69\% on SR without state alignment vs. +1.68\% on SR with state alignment). The full combination (d) achieves the best results. These findings confirm the effectiveness of state space learning and mid-state prediction. More ablation studies are provided in the appendix.



% \cref{tab:crosstask}, \cref{tab:coin} and \cref{tab:niv} show the comparisons between our method and others on CrossTask, COIN and NIV datasets. 



% Overall, our proposed method outperforms other methods by large margins on all the datasets and metrics. Specifically, for $T=3$ on CrossTask, our method outperforms P$^3$IV by over 8\% (31.83 vs. 23.34) on the sequence-level metric SR and outperforms EGPP by over 5\% (31.83 vs. 26.40). The improvements are consistent with longer procedures (\ie, $T=4,5,6$), and other two step-level metrics mAcc and mIoU. We also found that both P$^3$IV and EGPP didn't work well on COIN compared to their performances on CrossTask. Specifically, P$^3$IV outperforms DDN by large margins on CrossTask (\eg, 23.34 vs. 12.18 for SR with $T=3$). However, the improvements on COIN become marginal, especially for longer procedures (\ie, 11.32 vs. 11.13 for SR with $T=4$). The similar results are observed on EGPP, As comparisons, the SR of our SEPP is consistently larger than P3IV by over 16\% for $T=3$ and 10\% for $T=4$. The improvements of mACC and mIoU are also significant. These results demonstrate the better generality and effectiveness of our method on different datasets compared to P3IV and EGPP.

% % \input{tables/pdpp}
% % \input{tables/coin}

% \subsection{Results}\label{sec:stepcls}

% \textbf{Comparisons with Other Methods}. Tables~\ref{tab:sota} and~\ref{tab:coin} show the comparisons between our method and others on CrossTask and COIN datasets. Overall, our proposed method outperforms other methods by large margins on all the datasets and metrics. Specifically, for $T=3$ on CrossTask, our method outperforms P$^3$IV by over 8\% (31.83 vs. 23.34) on the sequence-level metric SR and outperforms EGPP by over 5\% (31.83 vs. 26.40). The improvements are consistent with longer procedures (\ie, $T=4,5,6$), and other two step-level metrics mAcc and mIoU. We also found that both P$^3$IV and EGPP didn't work well on COIN compared to their performances on CrossTask. Specifically, P$^3$IV outperforms DDN by large margins on CrossTask (\eg, 23.34 vs. 12.18 for SR with $T=3$). However, the improvements on COIN become marginal, especially for longer procedures (\ie, 11.32 vs. 11.13 for SR with $T=4$). The similar results are observed on EGPP, As comparisons, the SR of our SEPP is consistently larger than P3IV by over 16\% for $T=3$ and 10\% for $T=4$. The improvements of mACC and mIoU are also significant. These results demonstrate the better generality and effectiveness of our method on different datasets compared to P3IV and EGPP.

% An exception case is the recent work PDPP~\cite{wang2023pdpp} which achieves higher performance on both datasets. However, we argued that they define the start state and end state differently. Specifically, previous works define states as a 2-second window \textit{around} the start/end time, while PDPP defines the window \textit{after} the start time and \textit{before} the end time. Such a definition is more likely to access step information especially for short-term actions, leading to unfair comparisons with other methods. We further compared our method with PDPP under both conventional setting and their setting. The results on Table~\ref{tab:pdpp} show that our method outperforms PDPP with $T=3$ and $T=4$ under both settings, and the main improvement of PDPP comes from the different setting with a small $T$ (\eg, $\sim$11\% increase of SR on $T\!=\!3$). An interesting observation is that the benefits of different settings to PDPP become marginal with a larger $T$, which may be credited to their diffusion model.

% % \input{tables/ab_state}
% % \input{figures/similarity}

% \textbf{Ablation Studies}. We first conduct ablation studies on CrossTask to validate the effect of two key components, state alignment (Eq.~\ref{eq:lsenc} and Figure~\ref{fig:state}\textcolor{red}{(a)}) and mid-state prediction (Sec. \ref{sec:3.3.2}). As shown in Table~\ref{tab:abstate}, visual-language alignment improves SR by 1.4$\sim$2.4\% ((c) vs. (a), (d) vs. (b)) for $T=3$ on CrossTask. Also, the mid-state prediction module also improves the performance, and the improvements become larger with state alignment (\ie, +0.69\% on SR w/o state alignment vs. +1.68\% on SR with state alignment). The entire combination (d) achieves the best results. These results verified the impacts of state space learning and mid-state prediction. More ablation studies are in the appendix.



% \textbf{Qualitative Results}. Figure~\ref{fig:similarity} illustrates examples of state justifications, \ie, how the model aligns visual state observation with language descriptions. We retrieve top-5 similar descriptions from the corpus of state descriptions. Overall, the retrieved descriptions match the image well, and most of the top similar descriptions are aligned with the visual observations, which improved the explainable state understanding. More visualization results are in the appendix.



% \section{Experiments}

% In this section, we evaluate our PDPP model on three real-life datasets and show our competitive results for various planning horizons. We first present the result of our first training stage, which predicts the task class with the given observations in \cref{exp:task}. Then we compare our performance with other alternative approaches on the three datasets and demonstrate the effectiveness of our model in \cref{exp:compare}. We also study the role of task-supervision for our model in \cref{exp:ablation}. Finally, we show our prediction uncertainty evaluation results in \cref{exp:uncertainty}.

% \subsection{Implementation details}
% \label{exp:implementation}
% We use the basic U-Net~\cite{DBLP:conf/miccai/RonnebergerFB15} as our learnable model for projection diffusion, in which a modification is made by using a convolution operation along the planning horizon dimension for downsampling rather than max-pooling as in \cite{DBLP:conf/icml/JannerDTL22}. For training, we use the linear warm-up training scheme to optimize our model. We train different steps for the three datasets, corresponding to their scales.
% %All our experiments are conducted with ADAM\cite{DBLP:journals/corr/KingmaB14} on 8 NVIDIA TITAN Xp GPUs.
% More details about training and model architecture are provided in the supplementary material.

% \subsection{Evaluation protocol}
% \label{exp:protocol}

% \noindent \textbf{Datasets.} We evaluate our PDPP model on three instructional video datasets: \textbf{CrossTask} \cite{DBLP:conf/cvpr/ZhukovACFLS19}, \textbf{NIV} \cite{DBLP:conf/cvpr/AlayracBASLL16}, and \textbf{COIN} \cite{DBLP:conf/cvpr/TangDRZZZL019}. CrossTask contains 2,750 videos from 18 different tasks, with an average of 7.6 actions per video. The NIV dataset consists of 150 videos about 5 daily tasks, which has 9.5 actions in one video on average. COIN is much larger with 11,827 videos, 180 different tasks and 3.6 actions/video. We randomly select 70\% data for training and 30\% for testing as previous work~\cite{DBLP:conf/eccv/ChangHXAFN20,DBLP:conf/iccv/BiLX21,DBLP:conf/cvpr/0004HDDWJ22}. Note that we do not select 70\%/30\% for videos in each task, but in the whole dataset. Following previous work\cite{DBLP:conf/eccv/ChangHXAFN20,DBLP:conf/iccv/BiLX21,DBLP:conf/cvpr/0004HDDWJ22}, we extract all action sequences $\{[a_i, ..., a_{i+T-1}]\}_{i=1}^{n-T+1}$ with predicting horizon $T$ from the given video which contains $n$ actions by sliding a window of size $T$. Then for each action sequence $[a_i, ..., a_{i+T-1}]$, we choose the video clip feature at the beginning time of action $a_i$ and clip feature around the end time of $a_{i+T-1}$ as the start observation $o_s$ and goal state $o_g$, respectively. Both clips are 3 seconds long. For experiments conduct on CrossTask, we use two kinds of pre-extracted video features as the start and goal observations. One are the features provided in CrossTask dataset: each second of video content is encoded into a 3200-dimensional feature vector as a concatenation of the I3D, ResNet-152 and audio VGG features\cite{DBLP:conf/cvpr/HeZRS16,DBLP:conf/icassp/HersheyCEGJMPPS17,DBLP:conf/cvpr/CarreiraZ17}, which are also applied in \cite{DBLP:conf/iccv/BiLX21,DBLP:conf/eccv/ChangHXAFN20}. The other kind of features are generated by the encoder trained with the HowTo100M\cite{DBLP:conf/iccv/MiechZATLS19} dataset, as in \cite{DBLP:conf/cvpr/0004HDDWJ22}. For experiments on the other two datasets, we follow \cite{DBLP:conf/cvpr/0004HDDWJ22} to use the HowTo100M features for a fair comparison.

% Previous approaches~\cite{DBLP:conf/cvpr/0004HDDWJ22,DBLP:conf/iccv/BiLX21} compute the mIoU metric on every mini-batch (batch size larger than one) and calculate the average as the result. This brings a problem that the mIoU value can be influenced heavily by batch size. Consider if we set batch size is equal to the size of training data, then all predicted actions can be involved in the ground truth set and thus be correct predictions. However, if batch size is set to one, then any predicted action that not appears in the corresponding ground truth action sequence will be wrong. To address this problem, we standardize the way to get mIoU as computing IoU on every single sequence and calculating the average of these IoUs as the result (equal to setting of batch size $= 1$).

% ~\\ \noindent \textbf{Baselines.} Models for procedure planning\cite{DBLP:conf/eccv/ChangHXAFN20,DBLP:conf/iccv/BiLX21,DBLP:conf/cvpr/0004HDDWJ22} and other fully supervised planning approaches\cite{DBLP:conf/iccvw/FarhaG19, DBLP:conf/cvpr/EhsaniBRMF18, DBLP:conf/icml/SrinivasJALF18} are all involved in our comparison. Descriptions for these methods are available in the supplementary material.

% \begi

% The first stage of our learning is to predict the task class with the given start and goal observations. We implement this with MLP models and the detailed first-stage training process is described in the supplementary material.
% %We implement this with a two-layer Res-MLP\cite{DBLP:journals/corr/abs-2105-03404} and train it with a simple mseloss. 
% The classification results for different planning horizons on three datasets are shown in \cref{table:classifier_cross}. We can see that our classifier can perfectly figure out the task class in the NIV dataset since only 5 tasks are involved. For larger datasets CrossTask and COIN, our model can make right predictions most of the time.


% \subsection{Comparison with other approaches}
% \label{exp:compare}

% We follow previous work\cite{DBLP:conf/cvpr/0004HDDWJ22} and compare our approach with other alternative methods on three datasets, across multiple prediction horizons.

% ~\\ \noindent \textbf{NIV and COIN.} \cref{table:coin_niv} shows our evaluation results on the other two datasets NIV and COIN, from which we can see that our approach remains to be the best performer for both datasets. Specifically, in the NIV dataset where mAcc is relatively high, our model raises the SR value by more than 6\% both for the two horizons and outperforms the previous best by more than 8\% on mAcc metric when $T=4$. As for the large COIN dataset where mAcc is low, our model significantly improves mAcc by more than 20\%. 

% All the results suggest that our model performs well across datasets with different scales.

% \subsection{Study on task supervision}
% \label{exp:ablation}

% In this section, we study the role of task supervision for our model. \cref{table:task} shows the results of learning with and without task supervision, which suggest that the task supervision is quite helpful to our learning. Besides, we find that task supervision helps more for learning in the COIN dataset.
% %and learning with better visual features.
% We assume the reason is that fitting the COIN dataset is hard to our model since the number of tasks in COIN is large. Thus the guidance of task class information is more important to COIN compared with the other two datasets. Notably, training our model without task supervision also achieves state-of-the-art performance on multiple metrics, which suggests the effective of our approach.

% \begin{table}[]
% \centering
% \resizebox*{1\linewidth}{!}{
% \renewcommand\arraystretch{1.1}
% \begin{tabular}{cccccccc}
% \hline
% \multirow{2}{*}{}    & \multicolumn{1}{l}{\multirow{2}{*}{Dataset}}   & \multicolumn{3}{c}{w. task sup.} & \multicolumn{3}{c}{w.o. task sup.} \\ \cline{3-8} 
%                      &                            & SR$\uparrow$         & mAcc$\uparrow$      & mIoU$\uparrow$      & SR$\uparrow$        & mAcc$\uparrow$      & mIoU$\uparrow$      \\ \hline
% \multirow{4}{*}{$T$ = 3} & \multicolumn{1}{l}{CrossTask$_{Base}$}                  & \textbf{25.52}      & 53.43     & 56.90     & 22.82     & 51.56     & 54.36     \\
% & \multicolumn{1}{l}{CrossTask$_{How}$}                  & \textbf{37.20}      & 64.67     & 66.57     & 35.69     & 63.91     & 66.04     \\
%                      & \multicolumn{1}{l}{NIV}                        & \textbf{30.20}      & 48.45     & 57.28     & 28.37     & 45.96     & 54.31     \\
%                      & \multicolumn{1}{l}{COIN}                       & \textbf{21.33}      & 45.62     & 51.82     & 16.48     & 36.57     & 43.48     \\ \hline
% \multirow{4}{*}{$T$ = 4} & \multicolumn{1}{l}{CrossTask$_{Base}$}                  & \textbf{15.40}      & 49.42     & 56.99     & 14.91     & 49.55     & 56.28     \\
% & \multicolumn{1}{l}{CrossTask$_{How}$}                  & \textbf{21.48}      & 57.82     & 65.13     & 20.52     & 57.47     & 64.39     \\
%                      & \multicolumn{1}{l}{NIV}                        & \textbf{26.67}      & 46.89     & 59.45     & 26.50     & 46.08     & 58.94     \\
%                      & \multicolumn{1}{l}{COIN}                       & \textbf{14.41}      & 44.10     & 51.39     & 11.65     & 35.04     & 41.75     \\ \hline
% \multirow{2}{*}{$T$ = 5}    & \multicolumn{1}{l}{CrossTask$_{Base}$} & \textbf{9.37}      & 45.93     & 56.32     & 8.95     & 45.77     & 56.34     \\
% & \multicolumn{1}{l}{CrossTask$_{How}$} & \textbf{13.45}      & 54.01     & 65.32    & 12.80     & 53.44     & 64.01     \\ \hline
% \multirow{2}{*}{$T$ = 6}    & \multicolumn{1}{l}{CrossTask$_{Base}$} & \textbf{6.76}      & 43.61     & 57.51     & 6.06     & 44.15     & 57.07     \\
% & \multicolumn{1}{l}{CrossTask$_{How}$} & \textbf{8.41}      & 49.65     & 64.70     & 8.15     & 50.45     & 64.13     \\ \hline
% \end{tabular}}
% \caption{Ablation study on the role of task supervision. The $w.$ $task$ $sup.$ denotes learning with task supervision and $w.o.$ $task$ $sup.$ means training with the basic action labels only.}
% \label{table:task}
% % \vspace{-0.5cm}
% \end{table}

% \subsection{Evaluating probabilistic modeling}
% \label{exp:uncertainty}
% As discussed in \cref{sec::intro}, we introduce diffusion model to procedure planning to model the uncertainty in this problem. Here we follow \cite{DBLP:conf/cvpr/0004HDDWJ22} to evaluate our probabilistic modeling. We focus on CrossTask$_{How}$ as it has the most uncertainty for planning. Results on other datasets and further details are available in the supplement.

% Our model is probabilistic by starting from random noises and denoising step by step. We here introduce two baselines to compare with our diffusion based approach. We first remove the diffusion process in our method to establish the \textit{Noise} baseline, which just samples from a random noise with the given observations and task class condition in one shot. Then we further establish the \textit{Deterministic} baseline by setting the start distribution $\hat{x}_N = 0$, thus the model directly predicts a certain result with the given conditions. We reproduce the \textit{KL divergence, NLL, ModeRec} and \textit{ModePrec} in \cite{DBLP:conf/cvpr/0004HDDWJ22} and use these metrics along with \textit{SR} to evaluate our probabilistic model. The results in \cref{table:probabilistic1} and \cref{table:probabilistic2} suggest our approach has an excellent ability to model the uncertainty in procedure planning and can produce both diverse and reasonable plans(visualizations available in the supplement). Specifically, our approach improves \textit{ModeRec} greatly for longer horizons. There is less uncertainty when $T$ = 3, thus the diffusion based models performs worse than the deterministic one.
% \begin{table}[]
% \centering
% \resizebox*{0.8\linewidth}{!}{
% \renewcommand\arraystretch{1}
% \begin{tabular}{cccccc}
% \hline
% Metric$\downarrow$               & Model         & T = 3         & T=4           & T=5           & T=6           \\ \hline
% \multirow{3}{*}{NLL}  & Deterministic & \textbf{3.57} & 4.29          & 4.70          & 5.12          \\
%                      & Noise         & 3.58          & 4.04          & 4.45          & 4.79          \\
%                      & Ours          & 3.61          & \textbf{3.85} & \textbf{3.77} & \textbf{4.06} \\ \hline
% \multirow{3}{*}{KL-Div} & Deterministic & \textbf{2.99} & 3.40          & 3.54          & 3.82          \\
%                      & Noise         & 3.00          & 3.15          & 3.30          & 3.49          \\
%                      & Ours          & 3.03          & \textbf{2.96} & \textbf{2.62} & \textbf{2.76} \\ \hline
% \end{tabular}}
% \caption{Evaluation results of the plan distributions metrics.}
% \label{table:probabilistic1}
% \end{table}

% \begin{table}[]
% \centering
% \resizebox*{0.9\linewidth}{!}{
% \renewcommand\arraystretch{1}
% \begin{tabular}{cccccc}
% \hline
% Metric$\uparrow$            & Model         & T = 3          & T=4            & T=5            & T=6            \\ \hline
% \multirow{3}{*}{SR}       & Deterministic & \textbf{38.79} & 21.17          & 12.59          & 7.47           \\
%                           & Noise         & 34.92          & 18.99          & 12.04          & 7.82           \\
%                           & Ours          & 37.20          & \textbf{21.48} & \textbf{13.45} & \textbf{8.41}  \\ \hline
% \multirow{3}{*}{ModePrec} & Deterministic & \textbf{55.60} & \textbf{45.65} & 35.47          & 25.24          \\
%                           & Noise         & 51.04          & 43.90          & 34.35          & 24.51          \\
%                           & Ours          & 53.14          & 44.55          & \textbf{36.30} & \textbf{25.61} \\ \hline
% \multirow{3}{*}{ModeRec}  & Deterministic & 34.13          & 18.35          & 11.20          & 6.75           \\
%                           & Noise         & \textbf{39.42} & 25.56          & 15.67          & 11.04          \\
%                           & Ours          & 36.49          & \textbf{31.10} & \textbf{29.45} & \textbf{22.68} \\ \hline
% \end{tabular}}
% \caption{Evaluation results of diversity and accuracy metrics.}
% \label{table:probabilistic2}
% \vspace{-0.6cm}
% \end{table}
