\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}


\title{TIDM: Temporal Interpolation Diffusion Model for Procedure Planning}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Antiquus S.~Hippocampus, Natalia Cerebro \& Amelie P. Amygdale \thanks{ Use footnote for providing further information
about author (webpage, alternative address)---\emph{not} for acknowledging
funding agencies.  Funding acknowledgements go at the end of the paper.} \\
Department of Computer Science\\
Cranberry-Lemon University\\
Pittsburgh, PA 15213, USA \\
\texttt{\{hippo,brain,jen\}@cs.cranberry-lemon.edu} \\
\And
Ji Q. Ren \& Yevgeny LeNet \\
Department of Computational Neuroscience \\
University of the Witwatersrand \\
Joburg, South Africa \\
\texttt{\{robot,net\}@wits.ac.za} \\
\AND
Coauthor \\
Affiliation \\
Address \\
\texttt{email}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.
\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}
\maketitle
\begin{abstract}
     % 简述这篇paper主要的研究内容
     In this paper, we study the problem of procedure planning in instructional videos, which involves making goal-directed plans based on current visual observations in unstructured real-life videos.
     % 讲述进来的工作的主要内容
     Prior research has approached this as a distribution fitting problem, utilizing diffusion models to represent the entire sequence of actions, thereby transforming the planning challenge into one of sampling from this distribution.
     % 在这个基础上我们的工作是什么
     Building on this foundation, we introduce a novel approach by incorporating temporal diffusion model, where the temporal interpolation expands the previously non-existent temporal logical relationships.
     % 阐述插值器的细节
     In terms of details, we employ an interpolating predictor to guide the intermediate process within U-Net, using the start and end frames as inputs.
     This involves extracting potential features through an encoder and applying an interpolation strategy to derive potential features for the intermediate frames.
     % 更进一步
     Furthermore, to make sure the accuracy pf actions in outputs, we also add mask strategy both in inference and loss calculation.
     % Furthermore, to enhance the temporal logic between actions, we leverage a large language model (LLM) to guide the logical sequence of actions.
     % We validate our approach experimentally on the CrossTask, COIN, and NIV datasets. 
     Results across these three datasets of varying scales demonstrate that our TIDM model achieves state-of-the-art performance on several key metrics.
     The code and trained model are available at \href{https://www.example.com}{https://www.example.com}.
\end{abstract}

\section{Introduction}
%%%%%%%%%
% 引入，通过从其他任务或者现实情况中，
%%%%%%%%%

%%%%%%%%%%%
% 简要概括前人工作的重点，最后说明仍然存在的问题
%%%%%%%%%%
% 在程序规划领域，Zhao等人提出的P3IV和Wang等人开发的PDPP均通过减少对中间状态的依赖，前者利用自然语言指令作为弱监督信号，后者则采用扩散模型直接从初始和目标状态采样动作序列；Li等人提出的Skip-Plan方法则通过跳过不确定的状态预测并专注于动作预测来优化模型表现。与此同时，Niu等人提出的SCHEMA框架强调状态变化的重要性，并通过大型语言模型生成的状态描述来构建结构化的状态空间，而An-Lan Wang等人提出的EGPP模型采取事件引导方式填补观察状态与未观察动作间的语义鸿沟，Nagasinghe等人开发的KEPP系统通过知识增强的方法利用过程知识图谱来辅助模型决策，。尽管这些方法在不同程度上简化了标注流程并提高了模型的泛化能力，但它们仍然存在一些局限性，例如依赖于特定类型的数据集、在非标准环境下表现不稳定或需要额外的计算资源来生成描述性的状态表示。

% 在研究从教学视频中进行程序规划的相关工作中，学者们提出了多种方法来解决这一问题。早期的工作主要依赖于对中间步骤的视觉观察作为监督，这需要大量的标注工作。然而，近期的研究尝试通过不同的方法减少这种依赖性，并提高模型的泛化能力。 Chang等人提出了一种称为Dual Dynamics Networks(DDN)的方法，它利用状态与动作之间的共轭关系来学习可规划的潜在空间，从而解决了从无结构视频中学习结构化状态和动作空间的技术挑战。这种方法在实际的教学视频上进行了评估，并显示了其在程序规划任务中的优越性能。Bi等人则结合了贝叶斯推断和基于模型的模仿学习，提出了一个新的程序规划方法，该方法能够从收集的数据集中学习目标导向的动作，避免了需要在线与环境互动的问题，使其适用于服务机器人等现实应用。Zhao等人提出了一种弱监督方法P3IV，该方法通过自然语言指令来代替昂贵的时间视频标注，从而减少了标注工作量。他们的模型基于一个配备内存模块的Transformer，可以将起始和目标观测映射到一系列可能的动作序列上。此外，他们还引入了一个概率生成模块来捕捉程序规划中的不确定性。Niu等人认为状态变化对于程序规划至关重要，并提出了SCHEMA框架。该框架通过追踪程序中的状态变化，并利用大型语言模型生成的状态描述来更好地理解状态空间，进而改进程序规划。 Wang等人提出了一种投影扩散模型PDPP，用于处理程序规划问题。这种方法将整个中间动作序列分布建模为扩散过程，并通过简单的任务标签监督来训练模型，而不是使用中间视觉观察或自然语言指令作为监督。 Nagasinghe等人则提出了KEPP系统，这是一种知识增强的程序规划方法，它利用训练数据中提取的概率过程知识图谱来指导模型，从而帮助模型更好地理解和规划复杂的步骤顺序。 Li等人提出了Skip-Plan方法，该方法通过跳过不可靠的状态预测来提升程序规划中的模型性能。不同于之前专注于相邻动作之间链接的方法，他们将长链分解成多个可靠的子链，并设计了子链解码器来学习这些子链之间的判别关系。 An-Lan Wang等人则提出了一种事件引导的范式E3P，通过首先从观测状态中推断出事件，然后基于这些事件来规划动作，以克服观察到的视觉状态与未观察到的中间动作之间的语义差距问题。 上述方法展示了如何从不同角度解决程序规划问题，包括利用弱监督学习、引入不确定性建模、关注状态变化以及采用事件引导策略等。这些技术的发展促进了从教学视频中自动提取程序规划的能力，使得机器能够在更复杂的环境中执行任务。
%%%%%%%%%%%
% 说明本文主要的目的和改进点，通过引用一些跟我的创新点相关的paper，引出我想要做的东西
%%%%%%%%%%%%

%%%%%%%%%%%
% 介绍我的工作是什么，核心是什么，为什么这么做，具体解释。
%%%%%%%%%%%%

%%%%%%%%%%%%%%
% 总结我们的主要贡献
%%%%%%%%%%%



% 人类是程序规划的天然专家，即安排一系列的指令步骤以实现特定的目标。程序规划是具身人工智能系统的基本推理能力之一，在复杂的现实世界问题如机器人导航中至关重要（Tellex等人，2011；Jansen，2020；Brohan等人，2022）。程序任务中的指令步骤通常是修改对象状态的行为。例如，在“烤牛排”的任务中，生牛排在“调味牛排”之后会被撒上胡椒粉，然后放在烤架上，在“盖上盖子”之后，切割后成为熟肉片。这些前状态和后状态反映了实体的细粒度信息，如形状、颜色、大小和位置。因此，规划代理需要确定行动步骤之间的时序关系和步骤与状态之间的因果关系。
% 教学视频是学习日常任务中程序活动的自然资源。Chang等人（2020）提出了在教学视频中进行程序规划的问题，即给定开始和结束状态的视觉观察结果，生成一系列的动作步骤，如图1(a)所示。这个问题的动机在于学习一个结构化且可规划的状态和动作空间（Chang等人，2020）。而早期的工作（Chang等人，2020；Sun等人，2022；Bi等人，2021）利用了步骤序列和中间状态的完整注释作为监督（图1(b)），最近的工作（Zhao等人，2022；Wang等人，2023b）在较弱的监督设置下取得了有希望的结果，其中只有步骤序列注释可用于训练（图1(c)）。较弱的监督设置减少了视频注释的高昂成本，并验证了可规划动作空间的重要性。然而，由于训练和评估过程中排除了中间视觉状态，如何全面表示状态信息仍然是一个开放的问题。
% 本文旨在通过研究程序中步骤与状态之间的因果关系，建立更加结构化的状态空间来进行程序规划。我们首先问：人类是如何识别、理解和区分程序中的步骤的？除了关注动作信息外，人类会借助常识知识跟踪程序中的状态变化，这比仅查看动作更有信息量。例如，在“切片”之前，“烤牛排”是一个整体，在此步骤之后变成熟肉片。之前的自然语言处理研究表明，在各种推理任务中，如自动执行生物学实验（Mysore等人，2019）、烹饪食谱（Bollini等人，2013）和日常活动（Yang & Nyberg，2015）中，状态变化建模是有帮助的。最近的研究进一步明确地跟踪了程序文本中实体的状态变化（Mishra等人，2018；Tandon等人，2018；2020；Zhang等人，2023；Wu等人，2023；Li & Huang，2023）。状态变化建模的成功激励我们研究程序规划中步骤与状态之间的因果关系。
% 在这项工作中，我们通过将每个修改状态的步骤表示为状态变化来实现这一目标。我们的方法的核心是步骤表示和状态变化跟踪。对于步骤表示，受到大型语言模型（LLMs）在视觉识别领域成功的影响（Menon & Vondrick，2022），我们利用LLMs来描述每一步骤的状态变化。具体来说，我们要求LLMs（例如GPT-3.5）用我们设计的思考链提示来描述每一步骤之前和之后的状态（第3.2节）。对于状态变化跟踪，如图1(d)所示，我们通过跨模态对比学习将视觉状态观察与LLMs生成的语言描述对齐。直观上，初始视觉状态应该与第一步之前的状态描述对齐，而目标视觉状态应该与最后一步之后的状态描述对齐。由于语言描述比视觉状态更具辨别力，我们期望多模态对齐能学习到一个更结构化的状态空间。我们也把状态描述作为中间视觉状态的监督。最后，步骤预测模型以掩码标记预测的方式进行训练。
% 我们的主要贡献总结如下：
% - 我们指出状态变化对于教学视频中的程序规划至关重要，并提出了一种新的表示程序视频中步骤的方法，即将其表示为状态变化。
% - 我们提出通过将视觉状态观察与LLMs生成的语言描述对齐来跟踪状态变化，以获得更结构化的状态空间，并通过描述表示中间状态。
% - 我们在CrossTask、COIN和NIV数据集上的广泛实验表明了状态描述生成的质量和我们方法的有效性。
In recent years, the computer vision community has increasingly focused on the role of instructional video comprehension for model training [xxxxxx]. Instructional videos are Strong knowledge carriers, which contain rich scene variations and various actions. Learning procedural knowledge from instructional videos may be a natural ability for humans, but it is a major challenge for AI. This task requires AI to not only understand dynamic and complex scenes, but also to effectively segment and recognize key events, accurately identify and predict actions, and perform in-depth causal reasoning [xx,xx,xx]. Constructing such models can analyze complex human behaviors and provide effective assistance in problematic situations with clear goal orientation, such as home assistants, autonomous driving, and medical assistance tasks [xxxxxx]. To address the program planning problem in instructional videos, Han,Wu [PDPP] et al. proposed a solution to the PDPP model, which achieved good results through xxx, we we followed this work and made improvements, specifically xxxx, as shown in Fig. 1.

Previous instructional videos on program planning methods have typically treated it as a sequence planning problem, and most of the early work on program planning relied on two-branch autoregressive methods for stepwise prediction of intermediate states and actions [xx, xx, xx],along with the use of different network architectures for modeling probabilistic processes. A limitation of these methods is related to the autoregressive process, which is slow and subject to error propagation, and is prone to accumulate errors during complex planning. In contrast, zhao (to change the example) et al. proposed a single-branch non-autoregressive model that predicts intermediate steps in parallel, achieving good performance. However, this approach involves a complex training process of multiple loss functions to manage large design spaces. In contrast, we take inspiration from the work of [StoryDiffusion: Consistent Self-Attention for Long-Range Image and Video Generation] and others by using an interpolating predictor that uses the start and end frames as inputs, uses an encoder to extract the potential features of the intermediate frames to guide the intermediate process of Unet using the interpolation strategy, meanwhile we used LLM to guide the logical order of actions in order to enhance the temporal logic between actions, and achieved good results.

In this study, we propose an LLM-based mask interpolator that uses mask as the action mask, which limits the action range generation and directly restricts the search space for action prediction. Meanwhile, we use an interpolating predictor to extract potential features using encoder by using start and end frames as inputs, and utilize an interpolation strategy to get the potential features of intermediate frames to guide the intermediate process of Unet. At the same time, considering the possible adverse effects of inaccurate classification of task combinations in order to enhance the temporal logic between actions, we use use LLM to guide the logical order of actions. Finally, we performed experimental validation on three different types of datasets and obtained good results.

The main contributions of this paper are as follows:
\begin{enumerate}
     \item \textbf{An LLM-based mask interpolator} While inheriting the basic model of PDPP, this interpolator absorbs the experience of Mask Diffusion and innovatively uses masks to limit the range of actions, thus improving the accuracy of generated actions.

     \item \textbf{Using interpolated predictors and latent feature extraction methods} The model generation is enhanced by inputting start and end frames, extracting potential features using encoder, and generating potential features for intermediate frames through interpolation strategy to further guide the intermediate process of Unet.

     \item \textbf{Introduction of LLM to enhance the temporal logic of actions} In this paper, LLM is uniquely used to guide the logical sequence of actions, ensuring that the generated actions are rational and consistent in the time series.
     \item \textbf{Experimental validation was performed on different types of datasets}In this paper, we have conducted extensive experimental validation on CrossTask, COIN, and NIV datasets, and the results show that the method proposed in this paper significantly improves the model performance on several tasks.
\end{enumerate}

The remainder of the paper is organized as follows: Section 2 presents some preliminary knowledge; Section 3 contains the main results; Section 4 demonstrates the practicality of the results; Section 5 provides numerical examples; Section 6 presents the proofs; and finally, Section 7 concludes the paper.

\section{Related Work}
\label{gen_inst}
\textbf{Procedural video understanding}.The issue of program video comprehension is a growing concern.In [20](,Zhao et al. 2022),Zhao et al proposes a weakly supervised probabilistic procedure planning method for learning execution procedures from instructional videos. The method uses a Transformer-based model with a memory module to map the start and goal states to a sequence of possible actions.In [21](PDPP),Wang at all proposed PDPP method which effectively solves the process planning problem in instructional videos, and requires only task labels as supervision, avoiding reliance on intermediate visual or verbal annotations.In this paper, we follow this work by studying the problem of procedural video comprehension through learning goal-directed action planning and we improved the previous work by using LM-based mask interpolator to limit the
range of actions, thus improving the accuracy of generated actions.

\textbf{Diffusion probabilistic models.}
Nowadays, many reasearchers proved that utilizing diffusion models to represent the entire sequence of actions, thereby transforming the planning challenge into one of sampling have achieved great success[11,22,33,44].Further more,
diffusion probabilistic models have achieved great success in many research areas,such as image generation and editing[111],speech generation and processing[222],text generation [333] and many other domains.In this work, we apply diffusion process to procedure planning in instructional videos.Additionally, we employ an interpolating predictor to guide the intermediate process within Unet, using the start and end frames as inputs. This involves extracting potential features through an encoder and applying an interpolation strategy to derive potential features for the intermediate frames.

\textbf{Visual representation learning}Recently,the latest models usually use knowledge from the language domain (e.g., wikiHow) as distant supervision signals (Zhong et al. 2023; Zhou et al. 2023; Lin et al. 2022). However, the computational cost of training/fine-tuning large VLMs is usually prohibitively high. Alternatively, efforts have also been made to use pretrained large language models (LLMs) as a visual planner (Patel et al. 2023; Wang et al. 2023b), leveraging the zero- shot reasoning ability of powerful foundation models (Ge et al. 2023; Kim et al. 2022; OpenAI 2023; Touvron et al. 2023). However, significant performance gaps remain due to lack of domain knowledge.In this paper,we introduce LLM to guide the logical sequence of actions, ensuring that the generated actions are rational and consistent in the time series.

need a figure
\section{Method}
In this section, we present the details of our projected diffusion model for procedure planning (PDPP). We will first introduce the setup for this problem in Sec. 3.1. Then we present the diffusion model used to model the action sequence distribution in Sec. 3.2. To provide more precise conditional guidance both for the training and sampling process, a simple projection method is applied to our model, which we will discuss in Sec. 3.3. Finally, we show the training scheme (Sec. 3.4) and sampling process (Sec. 3.5) of our PDPP. An overview of PDPP is provided in Fig. 2.
\subsection{Problem formulation}
We follow the problem set-up of (PDPP),Wang at all:given an initial visual observation \(o_s\) and a target visual state \(o_g\), the model is tasked with generating a sequence of actions \(a_{1:T}\) that transforms the environment state from \(o_s\) to \(o_g\). Here, \(T\) represents the planning horizon, indicating the number of action steps the model must take, while \(\{o_s, o_g\}\) denotes two distinct states of the environment as shown in an instructional video.

We break down the procedure planning problem into two distinct sub-problems, as illustrated in Eq. (1). The first sub-problem involves learning task-related information \(c\) from the given pair \(\{o_s, o_g\}\). This step serves as an initial inference in the procedure planning process. The second sub-problem focuses on generating action sequences using the obtained task-related information and the given observations. It's important to note that Jing et al. [2] also decompose the procedure planning problem into two parts; however, their first sub-problem aims to provide long-horizon information for the second stage since they plan actions sequentially. In contrast, our approach seeks to establish conditions for sampling to facilitate more efficient learning.
\begin{equation}
     \begin{aligned}
          p\left(a_{1:T} \mid o_s, o_g\right) & = \int p\left(a_{1:T} \mid o_s, o_g, c\right) \, p\left(c \mid o_s, o_g\right) \, dc
     \end{aligned}
\end{equation}
During the training phase, we begin by training a basic model, represented by multi-layer perceptrons (MLPs), using the provided observations \(\{o_s, o_g\}\) to predict the task category. The task labels from the instructional videos, denoted as \(c\), are used to supervise the model's output. Following this, we evaluate \(p(a_{1:T} \mid o_s, o_g, c)\) in parallel with our model and utilize the ground truth (GT) intermediate action labels as supervision for training. Unlike the visual and language supervision used in previous studies, task label supervision is easier to obtain and results in a simpler learning process. During the inference phase, we use the initial and goal observations to predict the task class information \(c\), and then sample action sequences \(a_{1:T}\) from the learned distribution based on the given observations and the predicted \(c\), where \(T\) represents the planning horizon.


need figure 3
\subsection{Projected diffusion for procedure planning}
Our approach is composed of two main stages: predicting the task class and modeling the distribution of action sequences. The first stage involves solving a standard classification problem, which we address using a simple MLP model. The core component of our approach is the second stage, which focuses on modeling \(p(a_{1:T} \mid o_s, o_g, c)\) to effectively handle the procedure planning task. In contrast to Jing et al. [2], who frame this as a Goal-conditioned Markov Decision Process and use a policy \(p(a_t \mid o_t)\) alongside a transition model \(\tau_\mu(o_t \mid c, o_{t-1}, a_{t-1})\) to plan actions incrementally—an approach that is complex to train and slow during inference—we treat this problem as one of direct distribution fitting using a diffusion model.

\textbf{Diffusion model}.A diffusion model [19, 29] addresses the data generation challenge by representing the data distribution \(p(x_0)\) as a denoising Markov chain over a sequence of variables \(\{x_N, \ldots, x_0\}\), where \(x_N\) is assumed to follow a random Gaussian distribution. In the forward process, Gaussian noise is gradually added to the initial data \(x_0\), which can be described by \(q(x_n \mid x_{n-1})\). This allows for the generation of all intermediate noisy latent variables \(x_{1:N}\) across \(N\) diffusion steps. During the sampling phase, the diffusion model iteratively performs a denoising procedure \(p(x_{n-1} \mid x_n)\) over \(N\) iterations to approximate samples from the target data distribution. The forward and reverse processes of diffusion are depicted in Fig. 3.

In a typical diffusion model, the proportion of Gaussian noise introduced to the data at each diffusion step \(n\) is predetermined and denoted by \(\{\beta_n \in (0, 1)\}_{n=1}^N\). Each step of adding noise can be parameterized as:

\begin{align}
     x_n \mid x_{n-1} \sim \mathcal{N}\left( \sqrt{1 - \beta_n} \, x_{n-1}, \beta_n \mathbf{I} \right).
\end{align}

Since the hyper-parameters $\left\{\beta_n\right\}_{n=1}^N$ are predefined, the noise-adding process does not involve any training. Based on the discussion in [19], we can re-parameterize Eq. (2) to obtain:
\begin{equation}
     \begin{aligned}
          x_n = \sqrt{\bar{\alpha}_n} \, x_0 + \sqrt{1 - \bar{\alpha}_n} \, \epsilon
     \end{aligned}
     \quad \text{(1)}
\end{equation}
where $\bar{\alpha}_n = \prod_{s=1}^n \left(1 - \beta_s\right)$, and $\epsilon \sim \mathcal{N}(0, I)$.

In the denoising process, each step is parameterized as:
\begin{equation}
     \begin{aligned}
          p_\theta\left(x_{n-1} \mid x_n\right) = \mathcal{N}\left(x_{n-1} ; \mu_\theta\left(x_n, n\right), \Sigma_\theta\left(x_n, n\right)\right).
     \end{aligned}
     \quad \text{(1)}
\end{equation}

\subsection{Footnotes}

Indicate footnotes with a number\footnote{Sample of the first footnote} in the
text. Place the footnotes at the bottom of the page on which they appear.
Precede the footnote with a horizontal rule of 2~inches
(12~picas).\footnote{Sample of the second footnote}

\subsection{Figures}

All artwork must be neat, clean, and legible. Lines should be dark
enough for purposes of reproduction; art work should not be
hand-drawn. The figure number and caption always appear after the
figure. Place one line space before the figure caption, and one line
space after the figure. The figure caption is lower case (except for
first word and proper nouns); figures are numbered consecutively.

Make sure the figure caption does not get separated from the figure.
Leave sufficient space to avoid splitting the figure and figure caption.

You may use color figures.
However, it is best for the
figure captions and the paper body to make sense if the paper is printed
either in black/white or in color.
\begin{figure}[h]
     \begin{center}
          %\framebox[4.0in]{$\;$}
          \fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
     \end{center}
     \caption{Sample figure caption.}
\end{figure}

\subsection{Tables}

All tables must be centered, neat, clean and legible. Do not use hand-drawn
tables. The table number and title always appear before the table. See
Table~\ref{sample-table}.

Place one line space before the table title, one line space after the table
title, and one line space after the table. The table title must be lower case
(except for first word and proper nouns); tables are numbered consecutively.

\begin{table}[t]
     \caption{Sample table title}
     \label{sample-table}
     \begin{center}
          \begin{tabular}{ll}
               \multicolumn{1}{c}{\bf PART} & \multicolumn{1}{c}{\bf DESCRIPTION}
               \\ \hline \\
               Dendrite                     & Input terminal                      \\
               Axon                         & Output terminal                     \\
               Soma                         & Cell body (contains cell nucleus)   \\
          \end{tabular}
     \end{center}
\end{table}

\section{Conclusion}

\subsubsection*{Acknowledgments}
Use unnumbered third level headings for the acknowledgments. All
acknowledgments, including those to funding agencies, go at the end of the paper.


\bibliography{iclr2025_conference}
\bibliographystyle{iclr2025_conference}

\appendix
\section{Appendix}
You may include other additional sections here.


\end{document}
